{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retreival  \n",
    "\n",
    "Created by : FrancoisHUP\n",
    "Last update : 18 dec 2023\n",
    "\n",
    "We are doing indexation on TREC AP 88-90 documents and querying the system with 150 requests.\n",
    "Then we check the MAP score. \n",
    "\n",
    "0. Utils\n",
    "1. Config env\n",
    "2. Data loading\n",
    "3. Tokenization\n",
    "4. Indexing\n",
    "5. Search\n",
    "6. Evaluation \n",
    "\n",
    "It create an index for each tokenisation methode.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO config section where we install lucene and other requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read collection data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Reports Former Saigon Officials Released from Re-education Camp', 'text': \"More than 150 former officers of the\\noverthrown South Vietnamese government have been released from a\\nre-education camp after 13 years of detention, the official Vietnam\\nNews Agency reported Saturday.\\n   The report from Hanoi, monitored in Bangkok, did not give\\nspecific figures, but said those freed Friday included an\\nex-Cabinet minister, a deputy minister, 10 generals, 115\\nfield-grade officers and 25 chaplains.\\n   It quoted Col. Luu Van Ham, director of the Nam Ha camp south of\\nHanoi, as saying all 700 former South Vietnamese officials who had\\nbeen held at the camp now have been released.\\n   They were among 1,014 South Vietnamese who were to be released\\nfrom re-education camps under an amnesty announced by the Communist\\ngovernment to mark Tet, the lunar new year that begins Feb. 17.\\n   The Vietnam News Agency report said many foreign journalists and\\na delegation from the Australia-Vietnam Friendship Association\\nattended the Nam Ha release ceremony.\\n   It said Lt. Gen. Nguyen Vinh Nghi, former commander of South\\nVietnam's Third Army Corps, and Col. Tran Duc Minh, former director\\nof the Army Infantry Officers School, expressed ``gratitude to the\\ngovernment for its humane treatment in spite of the fact that most\\nof them (detainees) had committed heinous crimes against the\\ncountry and people.''\\n   The prisoners had been held without formal charges or trial\\nsince North Vietnam defeated the U.S.-backed South Vietnamese\\ngovernment in April 1975, ending the Vietnam War.\\n   Communist authorities had called the prisoners war criminals and\\nsaid they had to learn how to become citizens of the new society.\\n   Small numbers had been released occasionally without publicity\\nbut the government announced last year that 480 political prisoners\\nwould be freed to mark National Day on Sept. 2.\\n   On Thursday, Vice Minister of Information Phan Quang said 1,014\\nwould be released under the Tet amnesty.\\n   He reported a total of 150 prisoners would remain in the camps,\\nwhich he said once held 100,000.\\n   ``Depending on their repentance, they will gradually be released\\nwithin a short period of time,'' Quang said.\\n   He said many of the former inmates would return to their\\nfamilies in Ho Chi Minh City, formerly the South Vietnamese capital\\nof Saigon.\\n   The amnesties apparently are part of efforts by Communist Party\\nchief Nguyen Van Linh to heal internal divisions and improve\\nVietnam's image abroad.\"}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "doc_pattern = re.compile(r'<DOC>(.*?)</DOC>', re.DOTALL)\n",
    "docno_pattern = re.compile(r'<DOCNO>\\s*(.*?)\\s*</DOCNO>')\n",
    "head_pattern = re.compile(r'<HEAD>\\s*(.*?)\\s*</HEAD>')\n",
    "text_pattern = re.compile(r'<TEXT>\\s*(.*?)\\s*</TEXT>', re.DOTALL)\n",
    "\n",
    "def get_documents() :\n",
    "    \"\"\"\n",
    "    return a dictionary of documents with key = doc_id and value = {'title': title, 'text': text}. \n",
    "    output example :\n",
    "        documents[\"AP880212-0001\"] = \n",
    "        {'title': 'Reports Former Saigon Officials Released from Re-education Camp', \n",
    "        'text': \"More than 150 former officers of the\\noverthrown ...\"}\n",
    "    \"\"\"\n",
    "    documents_metadata = {}\n",
    "    # Get a list of all .gz files in the \"Ap\" directory\n",
    "    file_list = glob.glob('TREC AP 88-90/TREC AP 88-90/collection de documents/AP/*.gz') # start from ../src\n",
    "\n",
    "    # Loop over the list of files\n",
    "    for filename in file_list:\n",
    "        \n",
    "        # Open the .gz file\n",
    "        with gzip.open(filename, 'rt', encoding='latin1') as file:  # 'rt' mode for text reading\n",
    "            # Read the contents of the file\n",
    "            content = file.read()\n",
    "            for doc in doc_pattern.finditer(content):\n",
    "                doc_content = doc.group(1)\n",
    "\n",
    "                # Extracting individual elements\n",
    "                doc_id = docno_pattern.search(doc_content).group(1)\n",
    "                head = head_pattern.search(doc_content)\n",
    "                text = text_pattern.search(doc_content)\n",
    "                \n",
    "                documents_metadata[doc_id] = {\n",
    "                    'title': head.group(1) if head else 'Default Title',\n",
    "                    'text': text.group(1) if text else 'Default text'\n",
    "                }  \n",
    "\n",
    "    return documents_metadata\n",
    "\n",
    "documents = get_documents()\n",
    "print(documents[\"AP880212-0001\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of request :  150\n",
      "{'title': 'Antitrust Cases Pending', 'desc': 'Document discusses a pending antitrust case.'}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "topic_pattern = re.compile(r'<top>(.*?)</top>', re.DOTALL)\n",
    "# Regular expressions for individual elements\n",
    "num_pattern = re.compile(r'<num>\\s*Number:\\s*(\\d+)')\n",
    "title_pattern = re.compile(r'<title>\\s*Topic:\\s*(.*?)\\s*\\n')\n",
    "desc_pattern = re.compile(r'<desc>\\s*Description:\\s*(.*?)\\s*<narr>', re.DOTALL)\n",
    "\n",
    "def get_requests() :\n",
    "    \"\"\"\n",
    "    return a dictionary of requests with key = request_id and value = {'title': title, 'desc': desc}.\n",
    "    output example :\n",
    "        requests[\"001\"] = {'title': 'Antitrust Cases Pending', 'desc': 'Document discusses a pending antitrust case.'}\n",
    "    \"\"\"\n",
    "    requests_metadata = {}\n",
    "    \n",
    "    # Get a list of all topics files in the \"Topics-requetes\" directory\n",
    "    file_list = glob.glob('TREC AP 88-90/TREC AP 88-90/Topics-requetes/*') \n",
    "    # Loop over the list of files\n",
    "    for filename in file_list:\n",
    "\n",
    "        # Open the .gz file\n",
    "        with open(filename, 'r') as file:\n",
    "            # Read the content of the file\n",
    "            topic_requests_string = file.read()\n",
    "            for topic in topic_pattern.finditer(topic_requests_string):\n",
    "                topic_content = topic.group(1)\n",
    "\n",
    "                # Extracting individual elements\n",
    "                num = num_pattern.search(topic_content)\n",
    "                title = title_pattern.search(topic_content)\n",
    "                desc = desc_pattern.search(topic_content)\n",
    "                \n",
    "                if(num) :\n",
    "                    requests_metadata[num.group(1)] = {\n",
    "                        'title': title.group(1) if title else None,\n",
    "                        'desc': desc.group(1).strip() if desc else None\n",
    "                    }\n",
    "        \n",
    "    return requests_metadata \n",
    "requests = get_requests() \n",
    "print(\"Total number of request : \", len(requests))\n",
    "print(requests[\"001\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short request :  Antitrust Cases Pending\n",
      "Long request :  Antitrust Cases Pending Document discusses a pending antitrust case.\n"
     ]
    }
   ],
   "source": [
    "def build_request(req_lenght, requests) :\n",
    "  built_requests = {}\n",
    "  for request_id,request_data in requests.items() :  \n",
    "    request_string = request_data['title']\n",
    "    if req_lenght==\"long\" : \n",
    "      request_string += \" \" + request_data['desc']\n",
    "    built_requests[request_id] = request_string\n",
    "  return built_requests\n",
    "    \n",
    "# Build requests \n",
    "short_requests = build_request('short', requests)       \n",
    "long_requests = build_request('long', requests)   \n",
    "print(\"Short request : \", short_requests[\"001\"])    \n",
    "print(\"Long request : \", long_requests[\"001\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "JVM is already running and updating its classpath failed. Call initVM() instead just once but with a classpath keyword argument set to the module.CLASSPATH strings of all the JCC extension modules to be imported by this process",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import lucen and init VM. This should be done only once.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlucene\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlucene\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitVM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: JVM is already running and updating its classpath failed. Call initVM() instead just once but with a classpath keyword argument set to the module.CLASSPATH strings of all the JCC extension modules to be imported by this process"
     ]
    }
   ],
   "source": [
    "# Import lucen and init VM. This should be done only once.\n",
    "import lucene\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/frank/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download a stopword list from nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "print(len(nltk_stop_words))\n",
    "print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'sampl', 'document']\n"
     ]
    }
   ],
   "source": [
    "import java\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.analysis.tokenattributes import CharTermAttribute\n",
    "from org.apache.lucene.analysis.core import StopFilter\n",
    "from org.apache.lucene.analysis import CharArraySet\n",
    "from org.apache.lucene.analysis.en import PorterStemFilter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
    "    return lemmas #' '.join(lemmas)\n",
    "\n",
    "def tokenize(text, preprocess_method):\n",
    "    tokens = []\n",
    "    if preprocess_method == \"lemmatization\":\n",
    "        tokens = lemmatize(text)\n",
    "    else : \n",
    "        analyzer = StandardAnalyzer()\n",
    "        stream = analyzer.tokenStream(None, text)\n",
    "        # Stemming \n",
    "        if preprocess_method == \"stemming\":\n",
    "            stream = PorterStemFilter(stream)\n",
    "\n",
    "        # Stop words\n",
    "        stop_words_list = java.util.ArrayList()\n",
    "        for word in nltk_stop_words: #[\"and\", \"is\", \"the\", \"this\"]\n",
    "            stop_words_list.add(word)\n",
    "        stop_words = CharArraySet(stop_words_list, True)\n",
    "        stream = StopFilter(stream, stop_words)\n",
    "\n",
    "        term = stream.getAttribute(CharTermAttribute.class_)\n",
    "        stream.reset()\n",
    "\n",
    "        while stream.incrementToken():\n",
    "            tokens.append(term.toString())\n",
    "        stream.end()\n",
    "        analyzer.close()\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example of tokenization\n",
    "test_document = \"This is a sample document.\"\n",
    "tokens = tokenize(test_document, \"stemming\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base tokenisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Reports Former Saigon Officials Released from Re-education Camp', 'tokens': ['reports', 'former', 'saigon', 'officials', 'released', 'education', 'campmore', '150', 'former', 'officers', 'overthrown', 'south', 'vietnamese', 'government', 'released', 'education', 'camp', '13', 'years', 'detention', 'official', 'vietnam', 'news', 'agency', 'reported', 'saturday', 'report', 'hanoi', 'monitored', 'bangkok', 'give', 'specific', 'figures', 'said', 'freed', 'friday', 'included', 'ex', 'cabinet', 'minister', 'deputy', 'minister', '10', 'generals', '115', 'field', 'grade', 'officers', '25', 'chaplains', 'quoted', 'col', 'luu', 'van', 'ham', 'director', 'nam', 'ha', 'camp', 'south', 'hanoi', 'saying', '700', 'former', 'south', 'vietnamese', 'officials', 'held', 'camp', 'released', 'among', '1,014', 'south', 'vietnamese', 'released', 'education', 'camps', 'amnesty', 'announced', 'communist', 'government', 'mark', 'tet', 'lunar', 'new', 'year', 'begins', 'feb', '17', 'vietnam', 'news', 'agency', 'report', 'said', 'many', 'foreign', 'journalists', 'delegation', 'australia', 'vietnam', 'friendship', 'association', 'attended', 'nam', 'ha', 'release', 'ceremony', 'said', 'lt', 'gen', 'nguyen', 'vinh', 'nghi', 'former', 'commander', 'south', \"vietnam's\", 'third', 'army', 'corps', 'col', 'tran', 'duc', 'minh', 'former', 'director', 'army', 'infantry', 'officers', 'school', 'expressed', 'gratitude', 'government', 'humane', 'treatment', 'spite', 'fact', 'detainees', 'committed', 'heinous', 'crimes', 'country', 'people', 'prisoners', 'held', 'without', 'formal', 'charges', 'trial', 'since', 'north', 'vietnam', 'defeated', 'u.s', 'backed', 'south', 'vietnamese', 'government', 'april', '1975', 'ending', 'vietnam', 'war', 'communist', 'authorities', 'called', 'prisoners', 'war', 'criminals', 'said', 'learn', 'become', 'citizens', 'new', 'society', 'small', 'numbers', 'released', 'occasionally', 'without', 'publicity', 'government', 'announced', 'last', 'year', '480', 'political', 'prisoners', 'would', 'freed', 'mark', 'national', 'day', 'sept', '2', 'thursday', 'vice', 'minister', 'information', 'phan', 'quang', 'said', '1,014', 'would', 'released', 'tet', 'amnesty', 'reported', 'total', '150', 'prisoners', 'would', 'remain', 'camps', 'said', 'held', '100,000', 'depending', 'repentance', 'gradually', 'released', 'within', 'short', 'period', 'time', 'quang', 'said', 'said', 'many', 'former', 'inmates', 'would', 'return', 'families', 'ho', 'chi', 'minh', 'city', 'formerly', 'south', 'vietnamese', 'capital', 'saigon', 'amnesties', 'apparently', 'part', 'efforts', 'communist', 'party', 'chief', 'nguyen', 'van', 'linh', 'heal', 'internal', 'divisions', 'improve', \"vietnam's\", 'image', 'abroad']}\n"
     ]
    }
   ],
   "source": [
    "# 3m18s\n",
    "preprocess_base_docs = {}\n",
    "for doc_id, doc_data in documents.items():\n",
    "    preprocess_base_docs[doc_id] = {}\n",
    "    preprocess_base_docs[doc_id]['title'] = doc_data['title']\n",
    "    preprocess_base_docs[doc_id]['tokens'] = tokenize(doc_data['title'] + doc_data['text'], \"basic\")\n",
    "print(preprocess_base_docs[\"AP880212-0001\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization tokenisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Reports Former Saigon Officials Released from Re-education Camp', 'tokens': ['Reports', 'Former', 'Saigon', 'Officials', 'Released', 'from', 'Re-education', 'CampMore', 'than', '150', 'former', 'officer', 'of', 'the', 'overthrow', 'South', 'Vietnamese', 'government', 'have', 'be', 'release', 'from', 'a', 're-education', 'camp', 'after', '13', 'years', 'of', 'detention', ',', 'the', 'official', 'Vietnam', 'News', 'Agency', 'report', 'Saturday', '.', 'The', 'report', 'from', 'Hanoi', ',', 'monitor', 'in', 'Bangkok', ',', 'do', 'not', 'give', 'specific', 'figure', ',', 'but', 'say', 'those', 'free', 'Friday', 'include', 'an', 'ex-Cabinet', 'minister', ',', 'a', 'deputy', 'minister', ',', '10', 'general', ',', '115', 'field-grade', 'officer', 'and', '25', 'chaplains', '.', 'It', 'quote', 'Col.', 'Luu', 'Van', 'Ham', ',', 'director', 'of', 'the', 'Nam', 'Ha', 'camp', 'south', 'of', 'Hanoi', ',', 'as', 'say', 'all', '700', 'former', 'South', 'Vietnamese', 'officials', 'who', 'have', 'be', 'hold', 'at', 'the', 'camp', 'now', 'have', 'be', 'release', '.', 'They', 'be', 'among', '1,014', 'South', 'Vietnamese', 'who', 'be', 'to', 'be', 'release', 'from', 're-education', 'camp', 'under', 'an', 'amnesty', 'announce', 'by', 'the', 'Communist', 'government', 'to', 'mark', 'Tet', ',', 'the', 'lunar', 'new', 'year', 'that', 'begin', 'Feb.', '17', '.', 'The', 'Vietnam', 'News', 'Agency', 'report', 'say', 'many', 'foreign', 'journalists', 'and', 'a', 'delegation', 'from', 'the', 'Australia-Vietnam', 'Friendship', 'Association', 'attend', 'the', 'Nam', 'Ha', 'release', 'ceremony', '.', 'It', 'say', 'Lt.', 'Gen.', 'Nguyen', 'Vinh', 'Nghi', ',', 'former', 'commander', 'of', 'South', 'Vietnam', \"'s\", 'Third', 'Army', 'Corps', ',', 'and', 'Col.', 'Tran', 'Duc', 'Minh', ',', 'former', 'director', 'of', 'the', 'Army', 'Infantry', 'Officers', 'School', ',', 'express', '``', 'gratitude', 'to', 'the', 'government', 'for', 'its', 'humane', 'treatment', 'in', 'spite', 'of', 'the', 'fact', 'that', 'most', 'of', 'them', '(', 'detainees', ')', 'have', 'commit', 'heinous', 'crimes', 'against', 'the', 'country', 'and', 'people', '.', \"''\", 'The', 'prisoners', 'have', 'be', 'hold', 'without', 'formal', 'charge', 'or', 'trial', 'since', 'North', 'Vietnam', 'defeat', 'the', 'U.S.-backed', 'South', 'Vietnamese', 'government', 'in', 'April', '1975', ',', 'end', 'the', 'Vietnam', 'War', '.', 'Communist', 'authorities', 'have', 'call', 'the', 'prisoners', 'war', 'criminals', 'and', 'say', 'they', 'have', 'to', 'learn', 'how', 'to', 'become', 'citizens', 'of', 'the', 'new', 'society', '.', 'Small', 'number', 'have', 'be', 'release', 'occasionally', 'without', 'publicity', 'but', 'the', 'government', 'announce', 'last', 'year', 'that', '480', 'political', 'prisoners', 'would', 'be', 'free', 'to', 'mark', 'National', 'Day', 'on', 'Sept.', '2', '.', 'On', 'Thursday', ',', 'Vice', 'Minister', 'of', 'Information', 'Phan', 'Quang', 'say', '1,014', 'would', 'be', 'release', 'under', 'the', 'Tet', 'amnesty', '.', 'He', 'report', 'a', 'total', 'of', '150', 'prisoners', 'would', 'remain', 'in', 'the', 'camp', ',', 'which', 'he', 'say', 'once', 'hold', '100,000', '.', '``', 'Depending', 'on', 'their', 'repentance', ',', 'they', 'will', 'gradually', 'be', 'release', 'within', 'a', 'short', 'period', 'of', 'time', ',', \"''\", 'Quang', 'say', '.', 'He', 'say', 'many', 'of', 'the', 'former', 'inmates', 'would', 'return', 'to', 'their', 'families', 'in', 'Ho', 'Chi', 'Minh', 'City', ',', 'formerly', 'the', 'South', 'Vietnamese', 'capital', 'of', 'Saigon', '.', 'The', 'amnesty', 'apparently', 'be', 'part', 'of', 'efforts', 'by', 'Communist', 'Party', 'chief', 'Nguyen', 'Van', 'Linh', 'to', 'heal', 'internal', 'divisions', 'and', 'improve', 'Vietnam', \"'s\", 'image', 'abroad', '.']}\n"
     ]
    }
   ],
   "source": [
    "# 14m20s\n",
    "preprocess_lemme_docs = {}\n",
    "for doc_id, doc_data in documents.items():\n",
    "    preprocess_lemme_docs[doc_id] = {}\n",
    "    preprocess_lemme_docs[doc_id]['title'] = doc_data['title']\n",
    "    preprocess_lemme_docs[doc_id]['tokens'] = tokenize(doc_data['title'] + doc_data['text'], \"lemmatization\")\n",
    "print(preprocess_lemme_docs[\"AP880212-0001\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming tokenisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Reports Former Saigon Officials Released from Re-education Camp', 'tokens': ['report', 'former', 'saigon', 'offici', 'releas', 'educ', 'campmor', '150', 'former', 'offic', 'overthrown', 'south', 'vietnames', 'govern', 'releas', 'educ', 'camp', '13', 'year', 'detent', 'offici', 'vietnam', 'new', 'agenc', 'report', 'saturdai', 'report', 'hanoi', 'monitor', 'bangkok', 'give', 'specif', 'figur', 'said', 'freed', 'fridai', 'includ', 'ex', 'cabinet', 'minist', 'deputi', 'minist', '10', 'gener', '115', 'field', 'grade', 'offic', '25', 'chaplain', 'quot', 'col', 'luu', 'van', 'ham', 'director', 'nam', 'ha', 'camp', 'south', 'hanoi', 'sai', '700', 'former', 'south', 'vietnames', 'offici', 'held', 'camp', 'releas', 'among', '1,014', 'south', 'vietnames', 'releas', 'educ', 'camp', 'amnesti', 'announc', 'communist', 'govern', 'mark', 'tet', 'lunar', 'new', 'year', 'begin', 'feb', '17', 'vietnam', 'new', 'agenc', 'report', 'said', 'mani', 'foreign', 'journalist', 'deleg', 'australia', 'vietnam', 'friendship', 'associ', 'attend', 'nam', 'ha', 'releas', 'ceremoni', 'said', 'lt', 'gen', 'nguyen', 'vinh', 'nghi', 'former', 'command', 'south', \"vietnam'\", 'third', 'armi', 'corp', 'col', 'tran', 'duc', 'minh', 'former', 'director', 'armi', 'infantri', 'offic', 'school', 'express', 'gratitud', 'govern', 'human', 'treatment', 'spite', 'fact', 'detaine', 'commit', 'heinou', 'crime', 'countri', 'peopl', 'prison', 'held', 'without', 'formal', 'charg', 'trial', 'sinc', 'north', 'vietnam', 'defeat', 'u.', 'back', 'south', 'vietnames', 'govern', 'april', '1975', 'end', 'vietnam', 'war', 'communist', 'author', 'call', 'prison', 'war', 'crimin', 'said', 'learn', 'becom', 'citizen', 'new', 'societi', 'small', 'number', 'releas', 'occasion', 'without', 'public', 'govern', 'announc', 'last', 'year', '480', 'polit', 'prison', 'would', 'freed', 'mark', 'nation', 'dai', 'sept', '2', 'thursdai', 'vice', 'minist', 'inform', 'phan', 'quang', 'said', '1,014', 'would', 'releas', 'tet', 'amnesti', 'report', 'total', '150', 'prison', 'would', 'remain', 'camp', 'said', 'held', '100,000', 'depend', 'repent', 'gradual', 'releas', 'within', 'short', 'period', 'time', 'quang', 'said', 'said', 'mani', 'former', 'inmat', 'would', 'return', 'famili', 'ho', 'chi', 'minh', 'citi', 'formerli', 'south', 'vietnames', 'capit', 'saigon', 'amnesti', 'appar', 'part', 'effort', 'communist', 'parti', 'chief', 'nguyen', 'van', 'linh', 'heal', 'intern', 'divis', 'improv', \"vietnam'\", 'imag', 'abroad']}\n"
     ]
    }
   ],
   "source": [
    "#6m9s\n",
    "preprocess_stemme_docs = {}\n",
    "for doc_id, doc_data in documents.items():\n",
    "    preprocess_stemme_docs[doc_id] = {}\n",
    "    preprocess_stemme_docs[doc_id]['title'] = doc_data['title']\n",
    "    preprocess_stemme_docs[doc_id]['tokens'] = tokenize(doc_data['title'] + doc_data['text'], \"stemming\")\n",
    "print(preprocess_stemme_docs[\"AP880212-0001\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump variable into disk\n",
    "\n",
    "When tokenizing text, it create alot of memory which is store into the ram. \n",
    "At some point you want to clear the ram. Here is a way to dump the objects into a file.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open the file in write-binary mode and dump the variable into it\n",
    "with open(\"objects/preprocess_base_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocess_base_docs, f)\n",
    "\n",
    "with open(\"objects/preprocess_lemme_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocess_lemme_docs, f)\n",
    "\n",
    "with open(\"objects/preprocess_stemme_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocess_stemme_docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to retreive the variables object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# specify the file you want to load the variable from\n",
    "file = \"objects/preprocess_base_docs.pkl\"\n",
    "# open the file in read-binary mode and load the variable from it\n",
    "# with open(file, \"rb\") as f:\n",
    "#     preprocess_base_docs = pickle.load(f)\n",
    "\n",
    "# file = \"objects/preprocess_lemme_docs.pkl\"\n",
    "# with open(file, \"rb\") as f:\n",
    "#     preprocess_lemme_docs = pickle.load(f)\n",
    "\n",
    "file = \"objects/preprocess_stemme_docs.pkl\"\n",
    "with open(file, \"rb\") as f:\n",
    "    preprocess_stemme_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean existing index \n",
    "Delete existing index. If the index is not deleted, it will add more document to allready existing index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# delete the directory if it exists\n",
    "# if os.path.exists(\"index_base\"):\n",
    "#     shutil.rmtree(\"index_base\")\n",
    "# if os.path.exists(\"index_lemme\"):\n",
    "#     shutil.rmtree(\"index_lemme\")\n",
    "# if os.path.exists(\"index_stemme\"):\n",
    "#     shutil.rmtree(\"index_stemme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions\n",
    "from org.apache.lucene.store import MMapDirectory\n",
    "from org.apache.lucene.document import Document, Field, FieldType\n",
    "\n",
    "# Create an on-disk index using MMapDirectory\n",
    "def add_doc(w, doc_id, title, content):\n",
    "    t1 = FieldType()\n",
    "    t1.setStored(True)\n",
    "    t1.setTokenized(False)\n",
    "    t1.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "\n",
    "    t2 = FieldType()\n",
    "    t2.setStored(False)\n",
    "    t2.setTokenized(True)\n",
    "    t2.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)\n",
    "\n",
    "    doc = Document()\n",
    "    doc.add(Field(\"doc_id\", doc_id, t1))\n",
    "    doc.add(Field(\"title\", title, t1))\n",
    "    doc.add(Field(\"contents\", content, t2))\n",
    "    w.addDocument(doc)\n",
    "\n",
    "def index(preprocess_documents, index_path) :\n",
    "    index = MMapDirectory(index_path)\n",
    "    analyzer = StandardAnalyzer()\n",
    "    config = IndexWriterConfig(analyzer)\n",
    "    writer = IndexWriter(index, config)\n",
    "\n",
    "    try : \n",
    "        for doc_id, preprocess_document in preprocess_documents.items(): \n",
    "            # Add some documents\n",
    "            tokens = ''.join(preprocess_document[\"tokens\"])\n",
    "            add_doc(writer, doc_id, preprocess_document[\"title\"], tokens)\n",
    "        # Commit and close the writer\n",
    "    except Exception as e:  \n",
    "        print(\"Exception when indexing document : \", doc_id)\n",
    "        print(e)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index base token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from java.nio.file import Paths\n",
    "index(preprocess_base_docs, Paths.get(\"index_basic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index lemme token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from java.nio.file import Paths\n",
    "index(preprocess_lemme_docs, Paths.get(\"index_lemmatization\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index stemme token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from java.nio.file import Paths\n",
    "index(preprocess_stemme_docs, Paths.get(\"index_stemming\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of documents register in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index contains 242918 documents.\n"
     ]
    }
   ],
   "source": [
    "from org.apache.lucene.index import DirectoryReader\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from java.nio.file import Paths\n",
    "\n",
    "# specify the path to the index directory\n",
    "index_dir = \"./index_basic/\"\n",
    "\n",
    "# create a directory object for the index directory\n",
    "dir = FSDirectory.open(Paths.get(index_dir))\n",
    "\n",
    "# open an index reader\n",
    "reader = DirectoryReader.open(dir)\n",
    "\n",
    "# get the number of documents in the index\n",
    "num_docs = reader.numDocs()\n",
    "\n",
    "print(f\"The index contains {num_docs} documents.\")\n",
    "\n",
    "# close the index reader\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from java.io import File\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search.similarities import ClassicSimilarity, BM25Similarity, LMJelinekMercerSimilarity, BooleanSimilarity\n",
    "from org.apache.lucene.index import  DirectoryReader\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "\n",
    "### SEARCHING ### \n",
    "def set_similirity_weight_schemat(searcher, weight_schemat) :\n",
    "    if weight_schemat == \"classic\" : \n",
    "        searcher.setSimilarity(ClassicSimilarity())\n",
    "    if weight_schemat == \"bm25\" : \n",
    "        searcher.setSimilarity(BM25Similarity(1.2,0.75))\n",
    "    if weight_schemat == \"LM\" : \n",
    "        searcher.setSimilarity(LMJelinekMercerSimilarity(0.7))\n",
    "    if weight_schemat == \"boolean\" : \n",
    "        searcher.setSimilarity(BooleanSimilarity())\n",
    "\n",
    "def search(request, preprocess_method, weight_schemat):\n",
    "    path = \"index_\" + preprocess_method + \"/\"\n",
    "    directory = FSDirectory.open(File(path).toPath())\n",
    "    analyzer = StandardAnalyzer()\n",
    "    searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "    set_similirity_weight_schemat(searcher, weight_schemat)\n",
    "    preprocess_request = \" \".join(tokenize(request, preprocess_method))\n",
    "\n",
    "    query_string = preprocess_request.replace(\"/\", \"\").replace(\"?\", \"\").replace(\"`\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    try :\n",
    "        query = QueryParser(\"contents\", analyzer).parse(query_string)\n",
    "        # print(\"QUERY : \", query.toString())\n",
    "    except Exception:\n",
    "        print(\"Error when parse '\", query_string, \"'\" )\n",
    "\n",
    "    # TODO replace 100000 with 1000\n",
    "    scoreDocs = searcher.search(query, 100000).scoreDocs\n",
    "\n",
    "    # Stats and explanation\n",
    "    # print(\"%s total matching documents.\" % len(scoreDocs))\n",
    "    # for scoreDoc in scoreDocs:\n",
    "    #     doc = searcher.doc(scoreDoc.doc)\n",
    "    #     print(doc.get(\"doc_id\"), doc.get(\"title\"), scoreDoc.score)\n",
    "    #     explanation = searcher.explain(query, scoreDoc.doc)\n",
    "    #     print(explanation)\n",
    "\n",
    "    return scoreDocs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a single request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request :  Acquisitions Document discusses a currently proposed acquisition involving a U.S.\n",
      "company and a foreign company.\n",
      "Number of results :  23424\n"
     ]
    }
   ],
   "source": [
    "request = long_requests[\"002\"]\n",
    "results=search(request, \"lemmatization\", \"bm25\") # lemmatization, stemming, basic | classic, bm25, LM, boolean\n",
    "print(\"Request : \", request)\n",
    "print(\"Number of results : \",len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Run Trec File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_searcher(preprocess_method) :\n",
    "    directory = FSDirectory.open(File(\"index_\" + preprocess_method + \"/\").toPath())\n",
    "    searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "    return searcher\n",
    "\n",
    "def write_run_file(results, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for result in results:\n",
    "            f.write(\" \".join(map(str, result)) + \"\\n\")\n",
    "\n",
    "# Run the search for each request\n",
    "def get_results(requests, dir_path, tokenization_method, weight_schemat, length) :\n",
    "    searcher = get_searcher(tokenization_method)\n",
    "    for request_id, request in requests.items():\n",
    "        # Run variables \n",
    "        run = []\n",
    "        trec_run = tokenization_method + \"_\" + length + \"_\" + weight_schemat + \"_\" + request_id + \".txt\"\n",
    "        \n",
    "        # Run the search\n",
    "        results = search(request, tokenization_method, weight_schemat)\n",
    "        for i, scoreDoc in enumerate(results):\n",
    "            doc = searcher.doc(scoreDoc.doc)\n",
    "            # QueryId, Q0, DocId, Rank, Score, RunId \n",
    "            run.append((request_id, \"Q0\", doc.get(\"doc_id\"), i+1, scoreDoc.score, trec_run))\n",
    "        \n",
    "        # Write the run file\n",
    "        trec_run_path = dir_path + trec_run\n",
    "        write_run_file(run, trec_run_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create run files\n",
    "\n",
    "This will create a run file for each request for each tokenization methode and for each similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of file created  for basic, classic and short :  0\n",
      "File created paths like : basic_short_classic_001.txt\n",
      "Number of file created  for basic, bm25 and short :  0\n",
      "File created paths like : basic_short_bm25_001.txt\n",
      "Number of file created  for basic, LM and short :  0\n",
      "File created paths like : basic_short_LM_001.txt\n",
      "Number of file created  for lemmatization, classic and short :  0\n",
      "File created paths like : lemmatization_short_classic_001.txt\n",
      "Number of file created  for lemmatization, bm25 and short :  0\n",
      "File created paths like : lemmatization_short_bm25_001.txt\n",
      "Number of file created  for lemmatization, LM and short :  0\n",
      "File created paths like : lemmatization_short_LM_001.txt\n",
      "Number of file created  for stemming, classic and short :  0\n",
      "File created paths like : stemming_short_classic_001.txt\n",
      "Number of file created  for stemming, bm25 and short :  0\n",
      "File created paths like : stemming_short_bm25_001.txt\n",
      "Number of file created  for stemming, LM and short :  0\n",
      "File created paths like : stemming_short_LM_001.txt\n",
      "Number of file created  for basic, classic and long :  0\n",
      "File created paths like : basic_long_classic_001.txt\n",
      "Number of file created  for basic, bm25 and long :  0\n",
      "File created paths like : basic_long_bm25_001.txt\n",
      "Number of file created  for basic, LM and long :  0\n",
      "File created paths like : basic_long_LM_001.txt\n",
      "Number of file created  for lemmatization, classic and long :  0\n",
      "File created paths like : lemmatization_long_classic_001.txt\n",
      "Number of file created  for lemmatization, bm25 and long :  0\n",
      "File created paths like : lemmatization_long_bm25_001.txt\n",
      "Number of file created  for lemmatization, LM and long :  0\n",
      "File created paths like : lemmatization_long_LM_001.txt\n",
      "Number of file created  for stemming, classic and long :  0\n",
      "File created paths like : stemming_long_classic_001.txt\n",
      "Number of file created  for stemming, bm25 and long :  0\n",
      "File created paths like : stemming_long_bm25_001.txt\n",
      "Number of file created  for stemming, LM and long :  0\n",
      "File created paths like : stemming_long_LM_001.txt\n"
     ]
    }
   ],
   "source": [
    "#6m22s\n",
    "res_dir_path = \"TREC AP 88-90/TREC AP 88-90/trec_run/\"\n",
    "for length in [\"short\", \"long\"] :\n",
    "    for tokenization_method in [\"basic\", \"lemmatization\", \"stemming\"] :\n",
    "        for weight_schemat in [\"classic\", \"bm25\", \"LM\"] :\n",
    "            request = short_requests if length == \"short\" else long_requests\n",
    "            get_results(request, res_dir_path, tokenization_method , weight_schemat, length)\n",
    "            # print number of files in the directory\n",
    "            print(\"Number of file created  for \" + tokenization_method + \", \" + weight_schemat + \" and \" + length + \" : \",  len(glob.glob(tokenization_method + \"_\" + length + \"_\" + weight_schemat + \"_*.txt\")))\n",
    "            print(\"File created paths like :\", tokenization_method + \"_\" + length + \"_\" + weight_schemat + \"_001.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get qrel stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file TREC AP 88-90/TREC AP 88-90/trec_qrels/qrel_001.txt contains 111 relevant documents.\n",
      "Total number of relevant documents :  111\n",
      "Mean of relevant documents per file :  111.0\n",
      "Min of relevant documents per file :  111  in  TREC AP 88-90/TREC AP 88-90/trec_qrels/qrel_001.txt\n",
      "Max of relevant documents per file :  111  in  TREC AP 88-90/TREC AP 88-90/trec_qrels/qrel_001.txt\n"
     ]
    }
   ],
   "source": [
    "# check for each file of qrel the number of relevant documents\n",
    "# list of file names\n",
    "# files = [f'TREC AP 88-90/TREC AP 88-90/trec_qrels/qrel_0{i}.txt' for i in range(1, 150)] \n",
    "files = [f'TREC AP 88-90/TREC AP 88-90/trec_qrels/qrel_00{i}.txt' for i in range(1, 2)] \n",
    "\n",
    "sum = 0\n",
    "min = 100000\n",
    "min_id = \"\"\n",
    "max = 0\n",
    "max_id = \"\"\n",
    "relevant_docids = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # initialize the count of relevant documents\n",
    "        relevant_count = 0\n",
    "\n",
    "        for line in f:\n",
    "            # split the line into columns\n",
    "            columns = line.split()\n",
    "\n",
    "            # get the last column\n",
    "            last_column = columns[-1]\n",
    "\n",
    "            # if the last column is '1', increment the count of relevant documents\n",
    "            if last_column == '1':\n",
    "                relevant_docids.append(columns[2])\n",
    "                relevant_count += 1\n",
    "                sum+=1\n",
    "\n",
    "        if min > relevant_count :\n",
    "            min = relevant_count\n",
    "            min_id = file\n",
    "        if max < relevant_count :\n",
    "            max = relevant_count\n",
    "            max_id = file\n",
    "        print(f'The file {file} contains {relevant_count} relevant documents.')\n",
    "\n",
    "print(\"Total number of relevant documents : \", sum)\n",
    "print(\"Mean of relevant documents per file : \", sum/len(files) if len(files) > 0 else 1)\n",
    "print(\"Min of relevant documents per file : \", min, \" in \", min_id)\n",
    "print(\"Max of relevant documents per file : \", max, \" in \", max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trec eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecEval, TrecQrel, TrecRun\n",
    "\n",
    "def trec_eval(qrel_path, run_path):\n",
    "    run = TrecRun(run_path)\n",
    "    qrel= TrecQrel(qrel_path)\n",
    "    te = TrecEval(run, qrel)\n",
    "\n",
    "    result = {}\n",
    "    result[\"num_ret\"] = te.get_retrieved_documents(per_query=False)\n",
    "    result[\"num_rel\"] = te.get_relevant_documents(per_query=False)\n",
    "    result[\"num_rel_ret\"] = te.get_relevant_retrieved_documents(per_query=False)\n",
    "    result[\"map\"] = te.get_map(depth=100, per_query=False, trec_eval=True) \n",
    "    for v in [5, 10, 15, 20, 30, 100, 200, 500, 1000]:\n",
    "        result[f\"P@{v}\"] = te.get_precision(depth=v, per_query=False, trec_eval=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "def format_res(overall_result) :\n",
    "  map_values = [inner_dict['map'] for inner_dict in overall_result.values()]\n",
    "  mean_map = sum(map_values) / len(map_values)\n",
    "  P_10 = [inner_dict['P@10'] for inner_dict in overall_result.values()]\n",
    "  mean_P_10 = sum(P_10) / len(P_10)\n",
    "  # 5, 10, 15, 20, 30, 100, 200, 500, 1000\n",
    "  return { \n",
    "    \"MMAP\" : mean_map, \n",
    "    \"P@10\" :mean_P_10,\n",
    "    \"num_ret\": overall_result[\"1\"][\"num_ret\"], \n",
    "    \"num_rel\": overall_result[\"1\"][\"num_rel\"],\n",
    "    \"num_rel_ret\" : overall_result[\"1\"][\"num_rel_ret\"],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_ret': 3, 'num_rel': 23, 'num_rel_ret': 1, 'map': 0.014492753623188404, 'P@5': 0.2, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.005, 'P@500': 0.002, 'P@1000': 0.001}\n",
      "{'num_ret': 154, 'num_rel': 38, 'num_rel_ret': 20, 'map': 0.11511419529454726, 'P@5': 0.2, 'P@10': 0.2, 'P@15': 0.3333333333333333, 'P@20': 0.3, 'P@30': 0.23333333333333334, 'P@100': 0.18, 'P@200': 0.1, 'P@500': 0.04, 'P@1000': 0.02}\n",
      "{'num_ret': 17, 'num_rel': 110, 'num_rel_ret': 9, 'map': 0.058568956296229026, 'P@5': 0.6, 'P@10': 0.5, 'P@15': 0.6, 'P@20': 0.45, 'P@30': 0.3, 'P@100': 0.09, 'P@200': 0.045, 'P@500': 0.018, 'P@1000': 0.009}\n",
      "{'num_ret': 17, 'num_rel': 200, 'num_rel_ret': 10, 'map': 0.03819642857142857, 'P@5': 0.6, 'P@10': 0.7, 'P@15': 0.6, 'P@20': 0.5, 'P@30': 0.3333333333333333, 'P@100': 0.1, 'P@200': 0.05, 'P@500': 0.02, 'P@1000': 0.01}\n",
      "{'num_ret': 538, 'num_rel': 390, 'num_rel_ret': 58, 'map': 0.012548748052542243, 'P@5': 0.4, 'P@10': 0.2, 'P@15': 0.26666666666666666, 'P@20': 0.2, 'P@30': 0.16666666666666666, 'P@100': 0.16, 'P@200': 0.14, 'P@500': 0.112, 'P@1000': 0.058}\n",
      "{'num_ret': 103, 'num_rel': 231, 'num_rel_ret': 21, 'map': 0.02688932850952764, 'P@5': 0.6, 'P@10': 0.4, 'P@15': 0.26666666666666666, 'P@20': 0.25, 'P@30': 0.2, 'P@100': 0.19, 'P@200': 0.105, 'P@500': 0.042, 'P@1000': 0.021}\n",
      "{'num_ret': 3, 'num_rel': 23, 'num_rel_ret': 1, 'map': 0.014492753623188404, 'P@5': 0.2, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.005, 'P@500': 0.002, 'P@1000': 0.001}\n",
      "{'num_ret': 4794, 'num_rel': 39, 'num_rel_ret': 2, 'map': 0.018803418803418806, 'P@5': 0.4, 'P@10': 0.2, 'P@15': 0.13333333333333333, 'P@20': 0.1, 'P@30': 0.06666666666666667, 'P@100': 0.02, 'P@200': 0.01, 'P@500': 0.004, 'P@1000': 0.002}\n",
      "{'num_ret': 2995, 'num_rel': 19, 'num_rel_ret': 16, 'map': 0.02390221449515556, 'P@5': 0.0, 'P@10': 0.0, 'P@15': 0.0, 'P@20': 0.0, 'P@30': 0.03333333333333333, 'P@100': 0.07, 'P@200': 0.04, 'P@500': 0.024, 'P@1000': 0.014}\n",
      "{'num_ret': 6668, 'num_rel': 6, 'num_rel_ret': 5, 'map': 0.020833333333333332, 'P@5': 0.0, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.005, 'P@500': 0.002, 'P@1000': 0.003}\n",
      "{'num_ret': 154, 'num_rel': 38, 'num_rel_ret': 20, 'map': 0.13387124821804647, 'P@5': 0.2, 'P@10': 0.4, 'P@15': 0.4, 'P@20': 0.35, 'P@30': 0.26666666666666666, 'P@100': 0.19, 'P@200': 0.1, 'P@500': 0.04, 'P@1000': 0.02}\n",
      "{'num_ret': 2575, 'num_rel': 25, 'num_rel_ret': 1, 'map': 0.02, 'P@5': 0.2, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.005, 'P@500': 0.002, 'P@1000': 0.001}\n",
      "{'num_ret': 17, 'num_rel': 110, 'num_rel_ret': 9, 'map': 0.058568956296229026, 'P@5': 0.6, 'P@10': 0.5, 'P@15': 0.6, 'P@20': 0.45, 'P@30': 0.3, 'P@100': 0.09, 'P@200': 0.045, 'P@500': 0.018, 'P@1000': 0.009}\n",
      "{'num_ret': 17, 'num_rel': 200, 'num_rel_ret': 10, 'map': 0.03819642857142857, 'P@5': 0.6, 'P@10': 0.7, 'P@15': 0.6, 'P@20': 0.5, 'P@30': 0.3333333333333333, 'P@100': 0.1, 'P@200': 0.05, 'P@500': 0.02, 'P@1000': 0.01}\n",
      "{'num_ret': 242, 'num_rel': 54, 'num_rel_ret': 7, 'map': 0.010981112759568045, 'P@5': 0.0, 'P@10': 0.1, 'P@15': 0.13333333333333333, 'P@20': 0.1, 'P@30': 0.1, 'P@100': 0.06, 'P@200': 0.035, 'P@500': 0.014, 'P@1000': 0.007}\n",
      "{'num_ret': 103, 'num_rel': 231, 'num_rel_ret': 21, 'map': 0.029631031251230382, 'P@5': 0.8, 'P@10': 0.4, 'P@15': 0.3333333333333333, 'P@20': 0.25, 'P@30': 0.2, 'P@100': 0.19, 'P@200': 0.105, 'P@500': 0.042, 'P@1000': 0.021}\n",
      "{'num_ret': 3, 'num_rel': 23, 'num_rel_ret': 1, 'map': 0.014492753623188404, 'P@5': 0.2, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.005, 'P@500': 0.002, 'P@1000': 0.001}\n",
      "{'num_ret': 4794, 'num_rel': 39, 'num_rel_ret': 2, 'map': 0.014957264957264956, 'P@5': 0.2, 'P@10': 0.2, 'P@15': 0.13333333333333333, 'P@20': 0.1, 'P@30': 0.06666666666666667, 'P@100': 0.02, 'P@200': 0.01, 'P@500': 0.004, 'P@1000': 0.002}\n",
      "{'num_ret': 2995, 'num_rel': 19, 'num_rel_ret': 16, 'map': 0.01875773932414037, 'P@5': 0.0, 'P@10': 0.0, 'P@15': 0.0, 'P@20': 0.0, 'P@30': 0.03333333333333333, 'P@100': 0.06, 'P@200': 0.035, 'P@500': 0.02, 'P@1000': 0.013}\n",
      "{'num_ret': 154, 'num_rel': 38, 'num_rel_ret': 20, 'map': 0.1402919591427183, 'P@5': 0.2, 'P@10': 0.4, 'P@15': 0.4, 'P@20': 0.3, 'P@30': 0.26666666666666666, 'P@100': 0.19, 'P@200': 0.1, 'P@500': 0.04, 'P@1000': 0.02}\n",
      "{'num_ret': 2575, 'num_rel': 25, 'num_rel_ret': 1, 'map': 0.013333333333333332, 'P@5': 0.2, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.005, 'P@500': 0.002, 'P@1000': 0.001}\n",
      "{'num_ret': 17, 'num_rel': 110, 'num_rel_ret': 9, 'map': 0.058568956296229026, 'P@5': 0.6, 'P@10': 0.5, 'P@15': 0.6, 'P@20': 0.45, 'P@30': 0.3, 'P@100': 0.09, 'P@200': 0.045, 'P@500': 0.018, 'P@1000': 0.009}\n",
      "{'num_ret': 17, 'num_rel': 200, 'num_rel_ret': 10, 'map': 0.03819642857142857, 'P@5': 0.6, 'P@10': 0.7, 'P@15': 0.6, 'P@20': 0.5, 'P@30': 0.3333333333333333, 'P@100': 0.1, 'P@200': 0.05, 'P@500': 0.02, 'P@1000': 0.01}\n",
      "{'num_ret': 538, 'num_rel': 390, 'num_rel_ret': 58, 'map': 0.012948476648837639, 'P@5': 0.2, 'P@10': 0.4, 'P@15': 0.26666666666666666, 'P@20': 0.2, 'P@30': 0.23333333333333334, 'P@100': 0.19, 'P@200': 0.14, 'P@500': 0.112, 'P@1000': 0.058}\n",
      "{'num_ret': 242, 'num_rel': 54, 'num_rel_ret': 7, 'map': 0.03035375118708452, 'P@5': 0.2, 'P@10': 0.3, 'P@15': 0.2, 'P@20': 0.2, 'P@30': 0.2, 'P@100': 0.07, 'P@200': 0.035, 'P@500': 0.014, 'P@1000': 0.007}\n",
      "{'num_ret': 122, 'num_rel': 235, 'num_rel_ret': 11, 'map': 0.010161347212337675, 'P@5': 0.2, 'P@10': 0.4, 'P@15': 0.26666666666666666, 'P@20': 0.25, 'P@30': 0.2, 'P@100': 0.1, 'P@200': 0.055, 'P@500': 0.022, 'P@1000': 0.011}\n",
      "{'num_ret': 2272, 'num_rel': 14, 'num_rel_ret': 1, 'map': 0.014285714285714287, 'P@5': 0.2, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.005, 'P@500': 0.002, 'P@1000': 0.001}\n",
      "{'num_ret': 103, 'num_rel': 231, 'num_rel_ret': 21, 'map': 0.026800404653488725, 'P@5': 0.6, 'P@10': 0.4, 'P@15': 0.26666666666666666, 'P@20': 0.25, 'P@30': 0.2, 'P@100': 0.19, 'P@200': 0.105, 'P@500': 0.042, 'P@1000': 0.021}\n",
      "{'num_ret': 7996, 'num_rel': 11, 'num_rel_ret': 2, 'map': 0.09090909090909091, 'P@5': 0.2, 'P@10': 0.1, 'P@15': 0.06666666666666667, 'P@20': 0.05, 'P@30': 0.03333333333333333, 'P@100': 0.01, 'P@200': 0.01, 'P@500': 0.004, 'P@1000': 0.002}\n",
      "{'num_ret': 29345, 'num_rel': 38, 'num_rel_ret': 20, 'map': 0.08905871921049384, 'P@5': 0.2, 'P@10': 0.2, 'P@15': 0.3333333333333333, 'P@20': 0.35, 'P@30': 0.26666666666666666, 'P@100': 0.13, 'P@200': 0.095, 'P@500': 0.04, 'P@1000': 0.02}\n",
      "{'num_ret': 11440, 'num_rel': 110, 'num_rel_ret': 36, 'map': 0.02979137957106133, 'P@5': 0.6, 'P@10': 0.4, 'P@15': 0.26666666666666666, 'P@20': 0.25, 'P@30': 0.16666666666666666, 'P@100': 0.07, 'P@200': 0.045, 'P@500': 0.018, 'P@1000': 0.009}\n",
      "{'num_ret': 12749, 'num_rel': 200, 'num_rel_ret': 16, 'map': 0.011523275031593776, 'P@5': 0.4, 'P@10': 0.2, 'P@15': 0.13333333333333333, 'P@20': 0.1, 'P@30': 0.06666666666666667, 'P@100': 0.07, 'P@200': 0.04, 'P@500': 0.02, 'P@1000': 0.011}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[280], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m run_paths \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(res_dir_path \u001b[38;5;241m+\u001b[39m tokenization_method \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m length \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m weight_schemat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(qrel_paths)) :\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print(\"qrel_path : \", qrel_paths[i])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# print(\"run_path : \", run_paths[i])\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtrec_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqrel_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     MMAP \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m     count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[279], line 4\u001b[0m, in \u001b[0;36mtrec_eval\u001b[0;34m(qrel_path, run_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrec_eval\u001b[39m(qrel_path, run_path):\n\u001b[0;32m----> 4\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mTrecRun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     qrel\u001b[38;5;241m=\u001b[39m TrecQrel(qrel_path)\n\u001b[1;32m      6\u001b[0m     te \u001b[38;5;241m=\u001b[39m TrecEval(run, qrel)\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/trectools/trec_run.py:22\u001b[0m, in \u001b[0;36mTrecRun.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/trectools/trec_run.py:64\u001b[0m, in \u001b[0;36mTrecRun.read_run\u001b[0;34m(self, filename, run_header)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Make sure the values are correctly sorted by score\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/pandas/core/frame.py:6938\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6931\u001b[0m         \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Series];\u001b[39;00m\n\u001b[1;32m   6932\u001b[0m         \u001b[38;5;66;03m# expected List[ndarray]\u001b[39;00m\n\u001b[1;32m   6933\u001b[0m         keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6934\u001b[0m             Series(k, name\u001b[38;5;241m=\u001b[39mname)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   6935\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m (k, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, by)\n\u001b[1;32m   6936\u001b[0m         ]\n\u001b[0;32m-> 6938\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mlexsort_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\n\u001b[1;32m   6940\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6941\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(by):\n\u001b[1;32m   6942\u001b[0m     \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[1;32m   6944\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_label_or_level_values(by[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/pandas/core/sorting.py:373\u001b[0m, in \u001b[0;36mlexsort_indexer\u001b[0;34m(keys, orders, na_position, key, codes_given)\u001b[0m\n\u001b[1;32m    370\u001b[0m     n \u001b[38;5;241m=\u001b[39m codes\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(codes) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     cat \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cat\u001b[38;5;241m.\u001b[39mcategories)\n\u001b[1;32m    375\u001b[0m     codes \u001b[38;5;241m=\u001b[39m cat\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/pandas/core/arrays/categorical.py:388\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[43mCategoricalDtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_values_or_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# At this point, dtype is always a CategoricalDtype, but\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# we may have dtype.categories be None, and we need to\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# infer categories in a factorization step further below\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/pandas/core/dtypes/dtypes.py:328\u001b[0m, in \u001b[0;36mCategoricalDtype._from_values_or_dtype\u001b[0;34m(cls, values, categories, ordered, dtype)\u001b[0m\n\u001b[1;32m    321\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39m_from_categorical_dtype(\n\u001b[1;32m    322\u001b[0m         values\u001b[38;5;241m.\u001b[39mdtype, categories, ordered\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# If dtype=None and values is not categorical, create a new dtype.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# Note: This could potentially have categories=None and\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# ordered=None.\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mCategoricalDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(CategoricalDtype, dtype)\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/pandas/core/dtypes/dtypes.py:211\u001b[0m, in \u001b[0;36mCategoricalDtype.__init__\u001b[0;34m(self, categories, ordered)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ordered: Ordered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "qrel_paths = glob.glob('TREC AP 88-90/TREC AP 88-90/trec_qrels/*') \n",
    "\n",
    "res_dir_path = \"TREC AP 88-90/TREC AP 88-90/trec_run/\"\n",
    "MMAP = 0\n",
    "count = 0\n",
    "for length in [\"short\", \"long\"] :\n",
    "    for tokenization_method in [\"basic\", \"lemmatization\", \"stemming\"] :\n",
    "        for weight_schemat in [\"classic\", \"bm25\", \"LM\"] :\n",
    "            run_paths = glob.glob(res_dir_path + tokenization_method + \"_\" + length + \"_\" + weight_schemat + \"_*.txt\")\n",
    "            for i in range(0, len(qrel_paths)) :\n",
    "                # print(\"qrel_path : \", qrel_paths[i])\n",
    "                # print(\"run_path : \", run_paths[i])\n",
    "                results = trec_eval(qrel_paths[i], run_paths[i])\n",
    "                MMAP += results[\"map\"]\n",
    "                count+=1\n",
    "                if results[\"map\"] > 0.01 : \n",
    "                    print(run_paths[i])\n",
    "                    print(results)\n",
    "                if i == 149 :\n",
    "                    break\n",
    "print(\"MMAP : \", MMAP/count)\n",
    "\n",
    "\n",
    "# overall_result = trec_eval(qrel_paths, treq_run_paths)\n",
    "# result_formated=format_res(overall_result)\n",
    "# write_result({\"result_formated\" : result_formated,\"overall_result\" : overall_result},\"results/\" + test.title + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trashed code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of file names\n",
    "files = ['TREC AP 88-90/TREC AP 88-90/jugements de pertinence/qrels.1-50.AP8890.txt', \n",
    "         'TREC AP 88-90/TREC AP 88-90/jugements de pertinence/qrels.51-100.AP8890.txt',\n",
    "         'TREC AP 88-90/TREC AP 88-90/jugements de pertinence/qrels.101-150.AP8890.txt']\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            # split the line into columns\n",
    "            columns = line.split()\n",
    "\n",
    "            # get the first column and format it as a three-digit number\n",
    "            first_column = format(int(columns[0]), '03d')\n",
    "\n",
    "            # open the output file and write the line to it\n",
    "            with open(f'qrel_{first_column}.txt', 'a') as out_file:\n",
    "                out_file.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
