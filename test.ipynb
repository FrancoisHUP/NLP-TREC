{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from reader import get_documents, get_requests, get_judgement\n",
    "from test import test_list\n",
    "from tokenizer import Tokenizer\n",
    "from vectorizer import vectorize\n",
    "\n",
    "def eval(): \n",
    "  result = search(\"inverted index document\")\n",
    "  print(result)\n",
    "\n",
    "\n",
    "def run() :\n",
    "  # I initiate Tokenizer here to skip some step that otherwise would be redo multiple times.\n",
    "  tokenizer = Tokenizer()  \n",
    "  # Get documents \n",
    "  documents_metadata = get_documents()\n",
    "  # print(documents_metadata[list(documents_metadata.keys())[0]].title)\n",
    "  requests = get_requests()\n",
    "  judgements = get_judgement()\n",
    "\n",
    "  for test in test_list : \n",
    "\n",
    "    # Tokenize docs\n",
    "    preprocess_documents=tokenizer.tokenize(documents_metadata, test.pretreatment_type)\n",
    "    # print(\"document_tokens length :\", len(preprocess_documents))\n",
    "\n",
    "    # Vectorize docs\n",
    "    vector=vectorize(preprocess_documents, test.weight_schemat) # store vectors into database ? \n",
    "\n",
    "    # search \n",
    "    # query_vec = vectorizer.transform([query])\n",
    "    # cosine_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    # related_docs_indices = cosine_similarities.argsort()[:-len(documents)-1:-1]\n",
    "    # return related_docs_indices, cosine_similarities[related_docs_indices]\n",
    "    \n",
    "    # eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Dev\\Maitrise\\NLP\\Travail_Partie_1\\src\\test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Dev/Maitrise/NLP/Travail_Partie_1/src/test.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m run()\n",
      "\u001b[1;32mc:\\Dev\\Maitrise\\NLP\\Travail_Partie_1\\src\\test.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/Maitrise/NLP/Travail_Partie_1/src/test.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m preprocess_documents\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mtokenize(documents_metadata, test\u001b[39m.\u001b[39mpretreatment_type)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/Maitrise/NLP/Travail_Partie_1/src/test.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# print(\"document_tokens length :\", len(preprocess_documents))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/Maitrise/NLP/Travail_Partie_1/src/test.ipynb#W1sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/Maitrise/NLP/Travail_Partie_1/src/test.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Vectorize docs\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Dev/Maitrise/NLP/Travail_Partie_1/src/test.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m vector\u001b[39m=\u001b[39mvectorize(preprocess_documents, test\u001b[39m.\u001b[39;49mweight_schemat)\n",
      "File \u001b[1;32mc:\\Dev\\Maitrise\\NLP\\Travail_Partie_1\\src\\vectorizer.py:10\u001b[0m, in \u001b[0;36mvectorize\u001b[1;34m(preprocess_documents, weight_scheme)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvectorize\u001b[39m(preprocess_documents, weight_scheme):\n\u001b[0;32m      8\u001b[0m   \u001b[39mif\u001b[39;00m weight_scheme \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnormalized_frequency\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Apply the vectorizer on the documents\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     normalize_frequency \u001b[39m=\u001b[39m CountVectorizer()\u001b[39m.\u001b[39;49mfit_transform(preprocess_documents)\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Normalize the frequency vectors\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m normalize(normalize_frequency, norm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Dev\\Maitrise\\NLP\\Travail_Partie_1\\.venv\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Dev\\Maitrise\\NLP\\Travail_Partie_1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Dev\\Maitrise\\NLP\\Travail_Partie_1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1294112339.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    documents_metadata = get_documents()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from reader import get_documents, get_requests, get_judgement\n",
    "\n",
    "documents_metadata = get_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
