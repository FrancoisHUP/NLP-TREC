{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP880212-0001', 'Reports Former Saigon Officials Released from Re-education Camp', \"More than 150 former officers of the\\noverthrown South Vietnamese government have been released from a\\nre-education camp after 13 years of detention, the official Vietnam\\nNews Agency reported Saturday.\\n   The report from Hanoi, monitored in Bangkok, did not give\\nspecific figures, but said those freed Friday included an\\nex-Cabinet minister, a deputy minister, 10 generals, 115\\nfield-grade officers and 25 chaplains.\\n   It quoted Col. Luu Van Ham, director of the Nam Ha camp south of\\nHanoi, as saying all 700 former South Vietnamese officials who had\\nbeen held at the camp now have been released.\\n   They were among 1,014 South Vietnamese who were to be released\\nfrom re-education camps under an amnesty announced by the Communist\\ngovernment to mark Tet, the lunar new year that begins Feb. 17.\\n   The Vietnam News Agency report said many foreign journalists and\\na delegation from the Australia-Vietnam Friendship Association\\nattended the Nam Ha release ceremony.\\n   It said Lt. Gen. Nguyen Vinh Nghi, former commander of South\\nVietnam's Third Army Corps, and Col. Tran Duc Minh, former director\\nof the Army Infantry Officers School, expressed ``gratitude to the\\ngovernment for its humane treatment in spite of the fact that most\\nof them (detainees) had committed heinous crimes against the\\ncountry and people.''\\n   The prisoners had been held without formal charges or trial\\nsince North Vietnam defeated the U.S.-backed South Vietnamese\\ngovernment in April 1975, ending the Vietnam War.\\n   Communist authorities had called the prisoners war criminals and\\nsaid they had to learn how to become citizens of the new society.\\n   Small numbers had been released occasionally without publicity\\nbut the government announced last year that 480 political prisoners\\nwould be freed to mark National Day on Sept. 2.\\n   On Thursday, Vice Minister of Information Phan Quang said 1,014\\nwould be released under the Tet amnesty.\\n   He reported a total of 150 prisoners would remain in the camps,\\nwhich he said once held 100,000.\\n   ``Depending on their repentance, they will gradually be released\\nwithin a short period of time,'' Quang said.\\n   He said many of the former inmates would return to their\\nfamilies in Ho Chi Minh City, formerly the South Vietnamese capital\\nof Saigon.\\n   The amnesties apparently are part of efforts by Communist Party\\nchief Nguyen Van Linh to heal internal divisions and improve\\nVietnam's image abroad.\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "doc_pattern = re.compile(r'<DOC>(.*?)</DOC>', re.DOTALL)\n",
    "docno_pattern = re.compile(r'<DOCNO>\\s*(.*?)\\s*</DOCNO>')\n",
    "head_pattern = re.compile(r'<HEAD>\\s*(.*?)\\s*</HEAD>')\n",
    "text_pattern = re.compile(r'<TEXT>\\s*(.*?)\\s*</TEXT>', re.DOTALL)\n",
    "\n",
    "def get_documents() :\n",
    "    documents = []\n",
    "    # Get a list of all .gz files in the \"Ap\" directory\n",
    "    file_list = glob.glob('TREC AP 88-90/TREC AP 88-90/collection de documents/AP/*.gz') # start from ../src\n",
    "\n",
    "    # Loop over the list of files\n",
    "    for filename in file_list:\n",
    "        \n",
    "        # Open the .gz file\n",
    "        with gzip.open(filename, 'rt', encoding='latin1') as file:  # 'rt' mode for text reading\n",
    "            # Read the contents of the file\n",
    "            content = file.read()\n",
    "            for doc in doc_pattern.finditer(content):\n",
    "                doc_content = doc.group(1)\n",
    "\n",
    "                # Extracting individual elements\n",
    "                doc_id = docno_pattern.search(doc_content).group(1)\n",
    "                head = head_pattern.search(doc_content)\n",
    "                text = text_pattern.search(doc_content)\n",
    "                \n",
    "                documents.append([doc_id, head.group(1) if head else '',text.group(1) if text else ''])\n",
    "\n",
    "    return documents\n",
    "\n",
    "documents = get_documents()\n",
    "print(documents[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## co_occurrences analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# tokenise documents\n",
    "words = [nltk.word_tokenize(doc[2]) for doc in documents]\n",
    "\n",
    "# Define window size for co-occurrence\n",
    "window_size = 2\n",
    "\n",
    "# Create co-occurrence matrix\n",
    "co_occurrences = defaultdict(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    for j in range(i+1, min(i+1+window_size, len(words))):\n",
    "        w1, w2 = sorted([tuple(words[i]), tuple(words[j])])  # Convert lists to tuples and sort\n",
    "        if w1 != w2:  # Avoid self co-occurrences\n",
    "            co_occurrences[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print co-occurrence counts\n",
    "for w1, counts in co_occurrences.items():\n",
    "    for w2, count in counts.items():\n",
    "        if count > 1:\n",
    "            print(f\"Co-occurrence for ({w1}, {w2}): {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mutual Information analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv-linux/lib/python3.10/site-packages (1.26.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv-linux/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv-linux/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv-linux/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv-linux/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242918\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Vectorize the corpus\u001b[39;00m\n\u001b[1;32m     10\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m---> 11\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Keep as sparse matrix\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Get feature names\u001b[39;00m\n\u001b[1;32m     14\u001b[0m features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Dev/master/NLP/TREC/.venv-linux/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m         )\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import numpy as np\n",
    "\n",
    "# Vectorize the corpus\n",
    "# Convert list of lists to list of strings\n",
    "documents = [' '.join(doc) for doc in documents]\n",
    "\n",
    "# Vectorize the corpus\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)  # Keep as sparse matrix\n",
    "\n",
    "# Get feature names\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate MI for each pair of features (words)\n",
    "n_features = len(features)\n",
    "MI_matrix = np.zeros((n_features, n_features))\n",
    "for i in range(n_features):\n",
    "    for j in range(n_features):\n",
    "        MI_matrix[i, j] = mutual_info_score(X[:, i].toarray().ravel(), X[:, j].toarray().ravel())\n",
    "\n",
    "# Print MI values\n",
    "print(\"Mutual Information Matrix:\")\n",
    "print(MI_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install chardet\n",
    "!python3 -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "file_path = 'AP/AP880213'\n",
    "with open(file_path, 'rb') as file:\n",
    "    rawdata = file.read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    encoding = result['encoding']\n",
    "\n",
    "with open(file_path, 'r', encoding=encoding) as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/frank/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/frank/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words: [(',', 6508223), ('.', 5188065), ('<', 4915756), ('>', 4915756), ('said', 1446450), ('``', 1376639), (\"''\", 1349608), (\"'s\", 984940), ('$', 415036), (')', 364081)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))  # Use 'english' or another language\n",
    "\n",
    "# Path to the TREC AP 88-90 dataset directory\n",
    "dataset_path = 'AP/'\n",
    "\n",
    "# Initialize variables to store data\n",
    "doc_count = 0\n",
    "word_count = 0\n",
    "word_freq = Counter()\n",
    "\n",
    "# Iterate over the files in the dataset directory\n",
    "for filename in os.listdir(dataset_path):\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        rawdata = file.read()\n",
    "        result = chardet.detect(rawdata)\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    with open(file_path, 'r', encoding=encoding,  errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        filtered_words = [word for word in word_tokenize(content) if word.lower() not in stop_words]\n",
    "        word_freq.update(filtered_words)\n",
    "       \n",
    "\n",
    "# Calculate additional statistics\n",
    "# unique_word_count = len(word_freq)\n",
    "# average_words_per_doc = word_count / doc_count if doc_count else 0\n",
    "\n",
    "# Print the analysis\n",
    "# print(f\"Number of documents: {doc_count}\")\n",
    "# print(f\"Total word count: {word_count}\")\n",
    "# print(f\"Unique word count: {unique_word_count}\")\n",
    "# print(f\"Average words per document: {average_words_per_doc:.2f}\")\n",
    "print(f\"Top 10 most common words: {word_freq.most_common(10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    }
   ],
   "source": [
    "print(type(stop_words))\n",
    "stop_words.update([',','.','<','>','``',\"''\",\"'s\",'$',')','(',])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
